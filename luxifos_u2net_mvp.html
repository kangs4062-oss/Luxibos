<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Luxifos — Image Cleaner + Lighting (MVP with U²-Net / ONNX + Fallback)</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <style>
    html,body{height:100%;margin:0;font-family:Inter,system-ui,Segoe UI,Roboto,Helvetica,Arial}
    .app{display:grid;grid-template-columns:360px 1fr;height:100vh}
    .panel{padding:16px;border-right:1px solid #eee;box-sizing:border-box}
    .drop{height:220px;border:2px dashed #ccc;border-radius:8px;display:flex;align-items:center;justify-content:center;text-align:center;padding:16px;cursor:pointer}
    #preview{max-width:100%;display:block;margin-top:12px}
    #three-container{width:100%;height:100%;background:#222}
    label{display:block;margin-top:12px}
    .controls{margin-top:12px}
    .small{font-size:12px;color:#666}
  </style>
</head>
<body>
<div class="app">
  <div class="panel">
    <h2>Luxifos — MVP (U²-Net / ONNX + Fallback)</h2>
    <div id="drop" class="drop">Drop an image here or click to choose<br><small class="small">Uses U²-Net (ONNX) client-side when available; falls back to BodyPix for low-power devices. HDRI/IBL retained.</small></div>
    <input id="file" type="file" accept="image/*" style="display:none" />

    <canvas id="work" width="1024" height="1024" style="display:none"></canvas>
    <img id="preview" alt="preview" />

    <div class="controls">
      <label>Segmentation Engine: <select id="engine"><option value="onnx">U²-Net (ONNX)</option><option value="bodypix">BodyPix (fallback)</option></select></label>
      <label>Lighting Preset
        <select id="preset">
          <option value="studio">Studio</option>
          <option value="outdoor">Outdoor</option>
          <option value="dramatic">Dramatic</option>
          <option value="ibl_royal">IBL: Royal Esplanade</option>
          <option value="ibl_sunset">IBL: Sunset</option>
        </select>
      </label>

      <label>Exposure <input id="exposure" type="range" min="0" max="2" step="0.01" value="1"></label>
      <label>Roughness <input id="roughness" type="range" min="0" max="1" step="0.01" value="0.6"></label>
      <label>Env Intensity <input id="envIntensity" type="range" min="0" max="2" step="0.01" value="1"></label>
      <div style="margin-top:8px">
        <label class="small">Load your own HDRI (equirectangular .hdr): <input id="hdriFile" type="file" accept=".hdr,image/*"></label>
      </div>
      <button id="reset">Reset</button>

      <p id="status" class="small" style="margin-top:8px;color:#444">Loading segmentation engine...</p>
    </div>

    <p style="margin-top:12px;font-size:13px;color:#555">This MVP attempts to run U²-Net (converted to ONNX) in the browser using ONNX Runtime Web for high-quality mattes (hair/edges). If the ONNX model or WebGL/WASM execution fails, it falls back to BodyPix so the app remains usable on older devices. You must host the U²-Net ONNX file and point the `MODEL_URL` below to the hosted file.</p>
  </div>

  <div id="three-container"></div>
</div>

<!-- CDN libraries -->
<script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/three@0.158.0/build/three.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/three@0.158.0/examples/js/controls/OrbitControls.js"></script>
<script src="https://cdn.jsdelivr.net/npm/three@0.158.0/examples/js/loaders/RGBELoader.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix@2.0.5/dist/body-pix.js"></script>

<script>
(async function(){
  // Config: point this to a hosted u2net.onnx model (320x320 input). You must host this file (example: CDN, S3, Vercel).
  const MODEL_URL = 'https://your-cdn.example.com/models/u2net.onnx';

  // DOM
  const drop = document.getElementById('drop');
  const fileInput = document.getElementById('file');
  const preview = document.getElementById('preview');
  const work = document.getElementById('work');
  const ctx = work.getContext('2d');
  const preset = document.getElementById('preset');
  const exposure = document.getElementById('exposure');
  const roughnessInput = document.getElementById('roughness');
  const envIntensity = document.getElementById('envIntensity');
  const resetBtn = document.getElementById('reset');
  const hdriFile = document.getElementById('hdriFile');
  const engineSelect = document.getElementById('engine');
  const status = document.getElementById('status');

  drop.addEventListener('click',()=>fileInput.click());
  drop.addEventListener('dragover',e=>{e.preventDefault(); drop.style.borderColor='#66f'});
  drop.addEventListener('dragleave',e=>{e.preventDefault(); drop.style.borderColor='#ccc'});
  drop.addEventListener('drop',e=>{e.preventDefault(); drop.style.borderColor='#ccc'; handleFiles(e.dataTransfer.files)});
  fileInput.addEventListener('change',e=>handleFiles(e.target.files));

  function handleFiles(files){
    if(!files || files.length===0) return;
    const f = files[0];
    if(!f.type.startsWith('image/')) return alert('Please drop an image file');
    const url = URL.createObjectURL(f);
    loadImage(url).then(img=>processImage(img));
  }

  async function loadImage(url){
    return new Promise(res=>{ const img=new Image(); img.crossOrigin='anonymous'; img.onload=()=>res(img); img.src=url });
  }

  // Load BodyPix as fallback
  const bodyPixPromise = bodyPix.load({architecture:'MobileNetV1', outputStride:16, multiplier:0.75}).then(net=>({net})).catch(err=>{console.warn('BodyPix failed',err);return null});

  // Try to load ONNX Runtime and model
  let ortSession = null;
  let ortAvailable = false;
  try{
    status.textContent = 'Checking ONNX Runtime environment...';
    // prefer webgl then wasm
    const providers = ort.env.wasm.wasmPaths ? ['webgl','wasm'] : ['wasm'];
    // fetch model as arraybuffer
    const r = await fetch(MODEL_URL);
    if(!r.ok) throw new Error('Model not reachable: '+r.status);
    const modelArrayBuffer = await r.arrayBuffer();
    status.textContent = 'Creating ONNX session (this may take a second)...';
    ortSession = await ort.InferenceSession.create(modelArrayBuffer, {executionProviders: ['webgl','wasm']});
    ortAvailable = true;
    status.textContent = 'U²-Net (ONNX) loaded — high-quality segmentation enabled.';
    engineSelect.value = 'onnx';
  }catch(e){
    console.warn('ONNX load failed, falling back to BodyPix', e);
    status.textContent = 'ONNX unavailable — using BodyPix fallback. (You can host u2net.onnx and set MODEL_URL to enable ONNX.)';
    engineSelect.value = 'bodypix';
  }

  engineSelect.addEventListener('change',()=>{
    status.textContent = 'Engine set to: '+engineSelect.value;
  });

  async function processImage(img){
    const max = 1024;
    let w=img.width, h=img.height;
    const scale = Math.min(max/w, max/h, 1);
    w = Math.round(w*scale); h = Math.round(h*scale);
    work.width = w; work.height = h;
    ctx.clearRect(0,0,w,h);
    ctx.drawImage(img,0,0,w,h);

    let maskCanvas;
    if(engineSelect.value==='onnx' && ortAvailable && ortSession){
      try{
        status.textContent = 'Running U²-Net ONNX segmentation...';
        maskCanvas = await runU2NetONNX(work, ortSession);
      }catch(err){
        console.warn('ONNX inference failed, falling back', err);
        status.textContent = 'ONNX failed — falling back to BodyPix.';
        maskCanvas = await runBodyPix(work);
      }
    }else{
      maskCanvas = await runBodyPix(work);
    }

    // composite: apply mask alpha
    const out = document.createElement('canvas'); out.width=w; out.height=h; const octx = out.getContext('2d');
    octx.drawImage(img,0,0,w,h);
    const imgData = octx.getImageData(0,0,w,h);
    const maskCtx = maskCanvas.getContext('2d');
    const maskData = maskCtx.getImageData(0,0,w,h).data;
    for(let i=0, j=0;i<imgData.data.length;i+=4, j+=4){
      const a = maskData[j+3];
      if(a<128) imgData.data[i+3]=0; // threshold
    }
    octx.putImageData(imgData,0,0);
    preview.src = out.toDataURL('image/png');
    const blob = await new Promise(r=>out.toBlob(r,'image/png'));
    const texUrl = URL.createObjectURL(blob);
    setThreeTexture(texUrl, w, h);
  }

  // ---------- ONNX U2Net runner (best-effort) ----------
  // Note: U²-Net ONNX models vary. This runner assumes an input size of 320x320, single input name, and outputs a single sigmoid mask tensor.
  async function runU2NetONNX(canvas, session){
    // resize to 320x320 for model
    const modelSize = 320;
    const tmp = document.createElement('canvas'); tmp.width=modelSize; tmp.height=modelSize; const tctx = tmp.getContext('2d');
    tctx.drawImage(canvas, 0, 0, tmp.width, tmp.height);
    const imgData = tctx.getImageData(0,0,tmp.width,tmp.height).data;

    // preproc: HWC -> CHW, normalize 0-1 and maybe mean/std (U2Net often uses just /255)
    const floatData = new Float32Array(3 * modelSize * modelSize);
    for(let y=0;y<modelSize;y++){
      for(let x=0;x<modelSize;x++){
        const i = (y*modelSize + x)*4;
        const r = imgData[i] / 255.0;
        const g = imgData[i+1] / 255.0;
        const b = imgData[i+2] / 255.0;
        const idx = y*modelSize + x;
        floatData[idx] = r; // R plane
        floatData[modelSize*modelSize + idx] = g; // G plane
        floatData[2*modelSize*modelSize + idx] = b; // B plane
      }
    }

    const inputTensor = new ort.Tensor('float32', floatData, [1,3,modelSize,modelSize]);
    // common input names vary; try 'input', 'input.1', 'images', or fallback to first input name from session
    const inputNameCandidates = ['input','input.1','images','data'];
    let inputName = null;
    const sessInputNames = session.inputNames || (session._inputNames ? session._inputNames : []);
    for(const c of inputNameCandidates) if(sessInputNames.indexOf(c)!==-1) { inputName = c; break; }
    if(!inputName) inputName = sessInputNames.length ? sessInputNames[0] : 'input';

    const feeds = {};
    feeds[inputName] = inputTensor;
    const results = await session.run(feeds);
    // find first output
    const outName = Object.keys(results)[0];
    const outTensor = results[outName];
    let data = outTensor.data;
    // out may be [1,1,320,320] or [1,320,320]
    const outH = modelSize, outW = modelSize;
    // create mask canvas sized to original canvas
    const mask = document.createElement('canvas'); mask.width = canvas.width; mask.height = canvas.height; const mctx = mask.getContext('2d');
    const mImg = mctx.createImageData(outW, outH);
    for(let i=0;i<outW*outH;i++){
      const v = data[i]; // assuming single channel
      const alpha = Math.max(0, Math.min(255, Math.round(v*255))); // if sigmoid output [0,1]
      mImg.data[i*4] = 255; mImg.data[i*4+1]=255; mImg.data[i*4+2]=255; mImg.data[i*4+3]=alpha;
    }
    mctx.putImageData(mImg,0,0);
    // scale mask to canvas size
    const scaled = document.createElement('canvas'); scaled.width = canvas.width; scaled.height = canvas.height; const sctx = scaled.getContext('2d');
    sctx.imageSmoothingEnabled = true; sctx.drawImage(mask, 0, 0, scaled.width, scaled.height);
    return scaled;
  }

  // BodyPix fallback
  async function runBodyPix(canvas){
    const bp = await bodyPixPromise;
    if(!bp || !bp.net) throw new Error('BodyPix not available');
    status.textContent = 'Running BodyPix fallback segmentation...';
    const segmentation = await bp.net.segmentPerson(canvas, {internalResolution:'medium', segmentationThreshold:0.7});
    const mask = bodyPix.toMask(segmentation);
    // convert mask ImageData to canvas
    const m = document.createElement('canvas'); m.width = canvas.width; m.height = canvas.height; const mc = m.getContext('2d');
    const img = new ImageData(new Uint8ClampedArray(mask.data), canvas.width, canvas.height);
    mc.putImageData(img, 0, 0);
    return m;
  }

  // ---------- three.js scene with IBL (same as previous) ----------
  let renderer, scene, camera, controls, mesh, pmrem, currentEnv;
  const container = document.getElementById('three-container');

  function initThree(){
    renderer = new THREE.WebGLRenderer({antialias:true, alpha:true});
    renderer.setPixelRatio(window.devicePixelRatio); renderer.setSize(container.clientWidth, container.clientHeight);
    renderer.outputEncoding = THREE.sRGBEncoding; renderer.toneMapping = THREE.ACESFilmicToneMapping; renderer.toneMappingExposure = parseFloat(exposure.value);
    renderer.shadowMap.enabled = true; renderer.shadowMap.type = THREE.PCFSoftShadowMap;

    container.appendChild(renderer.domElement);

    scene = new THREE.Scene();
    camera = new THREE.PerspectiveCamera(45, container.clientWidth/container.clientHeight, 0.1, 1000);
    camera.position.set(0,0,2);

    controls = new THREE.OrbitControls(camera, renderer.domElement);
    controls.enableDamping = true;

    const ground = new THREE.Mesh(new THREE.PlaneGeometry(10,10), new THREE.MeshStandardMaterial({color:0x111111, roughness:1, metalness:0}));
    ground.rotation.x = -Math.PI/2; ground.position.y = -0.55; ground.receiveShadow = true; scene.add(ground);

    const hemi = new THREE.HemisphereLight(0xffffff,0x444444,0.6); scene.add(hemi);
    const dir = new THREE.DirectionalLight(0xffffff,0.6); dir.position.set(2,2,2); dir.castShadow=true; dir.shadow.radius=8; dir.shadow.mapSize.set(1024,1024); scene.add(dir);

    pmrem = new THREE.PMREMGenerator(renderer);
    pmrem.compileEquirectangularShader();

    window.addEventListener('resize',onResize);
    animate();
  }

  async function loadEnvFromURL(url){
    const rgbe = new THREE.RGBELoader();
    try{
      const tex = await new Promise((res,rej)=> rgbe.load(url, res, undefined, rej));
      const env = pmrem.fromEquirectangular(tex).texture;
      tex.dispose();
      setEnvironment(env);
    }catch(e){ console.warn('HDRI load failed', e); alert('Failed to load HDRI: '+e.message); }
  }

  function setEnvironment(env){
    if(currentEnv) currentEnv.dispose();
    currentEnv = env;
    scene.environment = env;
    scene.background = null;
    if(mesh && mesh.material){ mesh.material.envMap = env; mesh.material.needsUpdate = true; }
  }

  const presetEnvs = {
    'ibl_royal': 'https://threejs.org/examples/textures/equirectangular/royal_esplanade_1k.hdr',
    'ibl_sunset': 'https://threejs.org/examples/textures/equirectangular/sunset_1k.hdr'
  };

  function onResize(){ if(!camera || !renderer) return; camera.aspect = container.clientWidth/container.clientHeight; camera.updateProjectionMatrix(); renderer.setSize(container.clientWidth, container.clientHeight); }

  function setThreeTexture(url, w, h){ if(!renderer) initThree(); const loader = new THREE.TextureLoader(); loader.load(url, tex=>{ tex.encoding = THREE.sRGBEncoding; tex.flipY = false; tex.needsUpdate = true; const aspect = w/h; const geo = new THREE.PlaneGeometry(aspect, 1); const mat = new THREE.MeshStandardMaterial({map:tex, transparent:true, roughness:parseFloat(roughnessInput.value), metalness:0, envMapIntensity:parseFloat(envIntensity.value)}); if(mesh){ scene.remove(mesh); mesh.geometry.dispose(); mesh.material.dispose(); } mesh = new THREE.Mesh(geo, mat); mesh.castShadow=true; mesh.receiveShadow=false; mesh.position.y = 0; scene.add(mesh); camera.position.set(0,0,1.8); controls.target.set(0,0,0); controls.update(); }); }

  function animate(){ requestAnimationFrame(animate); if(renderer){ renderer.toneMappingExposure = parseFloat(exposure.value); if(mesh) mesh.material.roughness = parseFloat(roughnessInput.value); if(mesh) mesh.material.envMapIntensity = parseFloat(envIntensity.value); } controls.update(); renderer.render(scene, camera); }

  preset.addEventListener('change',()=>{
    const p = preset.value;
    if(p==='studio'){ renderer.toneMappingExposure = 1; scene.environment = null; }
    else if(p==='outdoor'){ renderer.toneMappingExposure = 1.3; scene.environment = null; }
    else if(p==='dramatic'){ renderer.toneMappingExposure = 0.7; scene.environment = null; }
    else if(p.startsWith('ibl_')){ const url = presetEnvs[p]; loadEnvFromURL(url); }
  });

  hdriFile.addEventListener('change', async (e)=>{ const f = e.target.files[0]; if(!f) return; const url = URL.createObjectURL(f); await loadEnvFromURL(url); });

  resetBtn.addEventListener('click',()=>{ exposure.value=1; roughnessInput.value=0.6; envIntensity.value=1; preset.value='studio'; if(scene) scene.environment=null; status.textContent='Engine: '+engineSelect.value; });

})();</script>
</body>
</html>
